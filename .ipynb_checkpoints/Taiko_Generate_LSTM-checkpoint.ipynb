{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supported-excess",
   "metadata": {},
   "source": [
    "# 讀取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personal-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  audio_prepare import *\n",
    "from map_prepare import getMapObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "headed-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=\"D:/osu/Songs\"\n",
    "allFolderList = os.listdir(root_path) #讀取所有的資料夾(歌曲)\n",
    "audio_list=[]\n",
    "map_list=[]\n",
    "for folder in allFolderList: #讀取資料夾的檔案\n",
    "    path=root_path+\"/\"+folder\n",
    "    audio_path=getAudioName(path) #取得音檔\n",
    "    audio_pms = readAudioPMSum(audio_path) #取得音檔以每毫秒的內容\n",
    "    audio_list.append(audio_pms)\n",
    "    map_list.append(getMapObj(path,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-discretion",
   "metadata": {},
   "source": [
    "## 資料標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "major-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extended-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min_Max_Scaler = preprocessing.MinMaxScaler( feature_range=(0,1) ) #建立0~1的Scaler\n",
    "audio_data=[]\n",
    "for audio in audio_list:\n",
    "    audio_data.append(Min_Max_Scaler.fit_transform(audio.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-geometry",
   "metadata": {},
   "source": [
    "## 圖譜處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "offshore-convention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 20, -1, '<s>', '<e>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0小紅 4大紅 8小藍  12大藍 20轉盤 -1空值\n",
    "hitObj_set=set()\n",
    "for osu_map in map_list:\n",
    "    for hit in osu_map:\n",
    "        hitObj_set.add(hit[1])\n",
    "hitObj = list(hitObj_set) #取得所有hitObject的類型\n",
    "hitObj.append(-1)\n",
    "hitObj.append(\"<s>\")\n",
    "hitObj.append(\"<e>\")\n",
    "hitObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "looking-arthritis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "np.zeros((2,len(hitObj)))\n",
    "if 4 in hitObj:\n",
    "    print(hitObj.index(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "varied-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立符合softmax的map\n",
    "#\n",
    "map_data=[]\n",
    "for i, audio in enumerate(audio_data):\n",
    "    audio_len = len(audio)+2 #音檔長度(將上開始結束)\n",
    "    map_sm = np.zeros((audio_len, len(hitObj))) #預計存softmax的輸出\n",
    "    for j, o in enumerate(map_sm): #預設每個為空物件\n",
    "        map_sm[j][-3]=1\n",
    "    map_sm[0][-2]=1 #設置頭尾\n",
    "    map_sm[0][-3]=0\n",
    "    map_sm[-1][-1]=1\n",
    "    map_sm[-1][-3]=0\n",
    "        \n",
    "    for ms, obj in map_list[i]:\n",
    "        map_sm[ms][-1]=0\n",
    "        map_sm[ms][hitObj.index(obj)]=1 #加入物件\n",
    "    map_data.append(map_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "phantom-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-providence",
   "metadata": {},
   "source": [
    "## 參數處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "circular-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([len(ad) for ad in audio_data])\n",
    "max_decoder_seq_length = max([len(md) for md in map_data])\n",
    "num_encoder_tokens = len(audio_data[0][0])\n",
    "num_decoder_tokens = len(map_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "altered-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定編碼器、解碼器input起始值(均為0矩陣)\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(audio_data), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(map_data), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(map_data), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unexpected-stand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 259528, 1) (12, 259530, 8) (12, 259530, 8)\n"
     ]
    }
   ],
   "source": [
    "for i, ad in enumerate(audio_data):\n",
    "    for t, tk in enumerate(ad):\n",
    "        encoder_input_data[i, t]=tk\n",
    "    for t, tk in enumerate(map_data[i]):\n",
    "        decoder_input_data[i, t]=tk\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t-1]=tk\n",
    "print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-banner",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "special-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "convinced-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # Batch size for training.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "latent_dim = 128  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "competitive-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "warming-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 8)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 66560       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  70144       input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8)      1032        lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 137,736\n",
      "Trainable params: 137,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "enhanced-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 127s 10s/step - loss: 1.0932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e432fe1f10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-chinese",
   "metadata": {},
   "source": [
    "## 取得編碼器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proper-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義編碼器取樣模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-yukon",
   "metadata": {},
   "source": [
    "## 取得解碼器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "least-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義解碼器的input\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 定義解碼器 LSTM 模型\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    \n",
    "# 以編碼器的記憶狀態 h 及 c 為解碼器的記憶狀態  \n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-campus",
   "metadata": {},
   "source": [
    "## 模型預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exciting-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_value = encoder_model.predict(encoder_input_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expanded-infrared",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1, 259528, 259528\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a37d8dc098d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Populate the first character of target sequence with the start character.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m output_tokens, h, c = decoder_model.predict(\n\u001b[0m\u001b[0;32m      5\u001b[0m             [target_seq] + states_value)\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[0;32m   1597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1599\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[0;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 259528, 259528\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "# Populate the first character of target sequence with the start character.\n",
    "target_seq[0, 0, -2] = 1.\n",
    "output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_seq)\n",
    "states_value[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0,-2] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-insertion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
